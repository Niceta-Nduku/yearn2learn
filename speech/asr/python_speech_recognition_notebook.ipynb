{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python_speech_recognition_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOk+d4vZ8uHcbvTZkcY0bFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scgupta/yearn2learn/blob/master/speech/asr/python_speech_recognition_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBo4UlPaq4O",
        "colab_type": "text"
      },
      "source": [
        "# Speech Recognition in Python\n",
        "\n",
        "There are several Automated Speech Recognition (ASR) alternatives, and most of them have bindings for Python. There are two kinds of solutions:\n",
        "\n",
        "- **Online:** These run on the cloud, and are accessed either through REST endpoints or Python library. Examples are services from Google, Amazon, Microsoft.\n",
        "- **Offline:** These run locally on the machine (i.e. no network connection required). These comes with flexibility or overhead (depending upon perspective) of needing to have either download pre-trained models, or you can train on your data as perneed. Examples are [CMU Sphinx](https://cmusphinx.github.io/), [Mozilla DeepSpeech](https://hacks.mozilla.org/2019/12/deepspeech-0-6-mozillas-speech-to-text-engine/), [Facebook wav2letter@anywhere](https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/).\n",
        "\n",
        "Speech Recognition APIs are of two types:\n",
        "- **Batch:** The full audio file is passed as parameter, and speech-to-text transcribing is done in one shot.\n",
        "- **Streaming:** The chunks of audio buffer are repeatedly passed on, and intermediate results are accessible.\n",
        "\n",
        "All packages support batch mode, and some support streaming mode too.\n",
        "\n",
        "One common use case is to collect audio from microphone and passes on the buffer to the speech recognition API. Invariably, in such transcribers, microphone is accessed though [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/), which is implemented over [PortAudio](http://www.portaudio.com/).\n",
        "\n",
        "From Colab menu, select: **Runtime** > **Change runtime type**, and verify that it is set to Python3, and select GPU if you want to try out GPU version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUjuKePpRJt5",
        "colab_type": "text"
      },
      "source": [
        "## Download audio files and create test cases\n",
        "\n",
        "Following files will be used as test cases for all ASR packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3_2z6qMRXcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/audio-0.6.0.tar.gz\n",
        "!tar -xvzf audio-0.6.0.tar.gz\n",
        "!ls -l ./audio/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbzYo01kRi8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TESTCASES = [\n",
        "  {\n",
        "    'filename': 'audio/2830-3980-0043.wav',\n",
        "    'text': 'experience proves this',\n",
        "    'encoding': 'LINEAR16',\n",
        "    'lang': 'en-US'\n",
        "  },\n",
        "  {\n",
        "    'filename': 'audio/4507-16021-0012.wav',\n",
        "    'text': 'why should one halt on the way',\n",
        "    'encoding': 'LINEAR16',\n",
        "    'lang': 'en-US'\n",
        "  },\n",
        "  {\n",
        "    'filename': 'audio/8455-210777-0068.wav',\n",
        "    'text': 'your power is sufficient i said',\n",
        "    'encoding': 'LINEAR16',\n",
        "    'lang': 'en-US'\n",
        "  }\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-23iCNLvBIx",
        "colab_type": "text"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujeuvj35Ksv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Tuple\n",
        "import wave\n",
        "\n",
        "def read_wav_file(filename) -> Tuple[bytes, int]:\n",
        "    with wave.open(filename, 'rb') as w:\n",
        "        rate = w.getframerate()\n",
        "        frames = w.getnframes()\n",
        "        buffer = w.readframes(frames)\n",
        "\n",
        "    return buffer, rate\n",
        "\n",
        "def simulate_stream(buffer: bytes, batch_size: int = 4096):\n",
        "    buffer_len = len(buffer)\n",
        "    offset = 0\n",
        "    while offset < buffer_len:\n",
        "        end_offset = offset + batch_size\n",
        "        buf = buffer[offset:end_offset]\n",
        "        yield buf\n",
        "        offset = end_offset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wFdQoEUQH-3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Google\n",
        "\n",
        "Google has only online speech-to-text ([documentation](https://cloud.google.com/speech-to-text/docs)) and supports both batch and stream modes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzxKbhqRV_jx",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install google cloud speech package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ot6pKWyWEmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install google-cloud-speech"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjcnbQvvY3Xu",
        "colab_type": "text"
      },
      "source": [
        "2. **Upload Google Cloud Cred file**\n",
        "\n",
        "Have Google Cloud creds stored in a file named `gc-creds.json`, and upload it by running following code cell. See https://developers.google.com/accounts/docs/application-default-credentials for more details.\n",
        "\n",
        "This may reqire enabling **third-party cookies**. Check out https://colab.research.google.com/notebooks/io.ipynb for other alternatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXquL3Y7bLQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emXbdQ1bTDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd\n",
        "!ls -l ./gc-creds.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msFTMyUWgtEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/gc-creds.json'\n",
        "\n",
        "!ls -l $GOOGLE_APPLICATION_CREDENTIALS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBsBFHWCMi",
        "colab_type": "text"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dlm4CWyQPeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import speech_v1\n",
        "from google.cloud.speech_v1 import enums\n",
        "\n",
        "def google_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    client = speech_v1.SpeechClient()\n",
        "\n",
        "    config = {\n",
        "        'language_code': lang,\n",
        "        'sample_rate_hertz': rate,\n",
        "        'encoding': enums.RecognitionConfig.AudioEncoding[encoding]\n",
        "    }\n",
        "\n",
        "    audio = {\n",
        "        'content': buffer\n",
        "    }\n",
        "\n",
        "    response = client.recognize(config, audio)\n",
        "    # For bigger audio file, the previous line can be replaced with following:\n",
        "    # operation = client.long_running_recognize(config, audio)\n",
        "    # response = operation.result()\n",
        "\n",
        "    for result in response.results:\n",
        "        # First alternative is the most probable result\n",
        "        alternative = result.alternatives[0]\n",
        "        return alternative.transcript\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('google-cloud-batch-stt: \"{}\"'.format(\n",
        "        google_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhaFWC7rN9b",
        "colab_type": "text"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9wMydkzrdX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import speech\n",
        "from google.cloud.speech import enums\n",
        "from google.cloud.speech import types\n",
        "\n",
        "def response_stream_processor(responses):\n",
        "    print('interim results: ')\n",
        "\n",
        "    transcript = ''\n",
        "    num_chars_printed = 0\n",
        "    for response in responses:\n",
        "        if not response.results:\n",
        "            continue\n",
        "\n",
        "        result = response.results[0]\n",
        "        if not result.alternatives:\n",
        "            continue\n",
        "\n",
        "        transcript = result.alternatives[0].transcript\n",
        "        print('{0}final: {1}'.format(\n",
        "            '' if result.is_final else 'not ',\n",
        "            transcript\n",
        "        ))\n",
        "\n",
        "    return transcript\n",
        "\n",
        "def google_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    config = types.RecognitionConfig(\n",
        "        encoding=enums.RecognitionConfig.AudioEncoding[encoding],\n",
        "        sample_rate_hertz=rate,\n",
        "        language_code=lang\n",
        "    )\n",
        "\n",
        "    streaming_config = types.StreamingRecognitionConfig(\n",
        "        config=config,\n",
        "        interim_results=True\n",
        "    )\n",
        "\n",
        "    audio_generator = simulate_stream(buffer)  # buffer chunk generator\n",
        "    requests = (types.StreamingRecognizeRequest(audio_content=chunk) for chunk in audio_generator)\n",
        "    responses = client.streaming_recognize(streaming_config, requests)\n",
        "    # Now, put the transcription responses to use.\n",
        "    return response_stream_processor(responses)\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('google-cloud-streaming-stt: \"{}\"'.format(\n",
        "        google_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K8uQUcB5OBA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Amazon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Fg2BE75Qoo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Microsoft Azure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7W8nsP45IUx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# CMU Sphinx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZEgZKG5cWg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Mozilla DeepSpeech\n",
        "\n",
        "Mozilla released [DeepSpeech 0.6](https://github.com/mozilla/DeepSpeech/releases/tag/v0.6.0) with [APIs in C, Java, .NET, Python, and JavaScript](https://deepspeech.readthedocs.io/en/v0.6.0/Python-API.html).\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. **Install DeepSpeech**\n",
        "\n",
        "You can install DeepSpeech with pip (make it deepspeech-gpu==0.6.0 if you are using GPU in colab runtime)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbWPbs_27f3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install deepspeech==0.6.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIe7haLO7yo4",
        "colab_type": "text"
      },
      "source": [
        "2. **Download and unzip models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT-n1jLj8Ff4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/deepspeech-0.6.0-models.tar.gz\n",
        "!tar -xvzf deepspeech-0.6.0-models.tar.gz\n",
        "!ls -l ./deepspeech-0.6.0-models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGaM4wp8Ykp",
        "colab_type": "text"
      },
      "source": [
        "3. **Test that it all works**\n",
        "\n",
        "Examine the output of the last three commands, and you will see results *“experience proof less”*, *“why should one halt on the way”*, and *“your power is sufficient i said”* respectively. You are all set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pPnZssj8fPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/2830-3980-0043.wav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxm5RE68zu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/4507-16021-0012.wav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hq_tEFQ8254",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/8455-210777-0068.wav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTcABJ2c9CRa",
        "colab_type": "text"
      },
      "source": [
        "## API: Create model object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU41WTEr9G-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import deepspeech\n",
        "\n",
        "model_file_path = 'deepspeech-0.6.0-models/output_graph.pbmm'\n",
        "beam_width = 500\n",
        "model = deepspeech.Model(model_file_path, beam_width)\n",
        "\n",
        "# Add language model for better accuracy\n",
        "lm_file_path = 'deepspeech-0.6.0-models/lm.binary'\n",
        "trie_file_path = 'deepspeech-0.6.0-models/trie'\n",
        "lm_alpha = 0.75\n",
        "lm_beta = 1.85\n",
        "model.enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB4wl_9P9ilW",
        "colab_type": "text"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTaKt_rm9wY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def deepspeech_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    data16 = np.frombuffer(buffer, dtype=np.int16)\n",
        "    return model.stt(data16)\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('deepspeech-batch-stt: \"{}\"'.format(\n",
        "        deepspeech_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3jT8NR-qGb",
        "colab_type": "text"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU7lHQ2A-svH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deepspeech_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "\n",
        "    # Create stream\n",
        "    context = model.createStream()\n",
        "\n",
        "    text = ''\n",
        "    for chunk in audio_generator:\n",
        "        data16 = np.frombuffer(chunk, dtype=np.int16)\n",
        "        # feed stream of chunks\n",
        "        model.feedAudioContent(context, data16)\n",
        "        interim_text = model.intermediateDecode(context)\n",
        "        if interim_text != text:\n",
        "            text = interim_text\n",
        "            print('inetrim result: {}'.format(text))\n",
        "\n",
        "    # get final resut and close stream\n",
        "    text = model.finishStream(context)\n",
        "    return text\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('deepspeech-streaming-stt: \"{}\"'.format(\n",
        "        deepspeech_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74sXAsSL5v8T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Facebook wav2letter@anywhere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aqlb4wEcdOx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# SpeechRecognition Package\n",
        "\n",
        "The [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) package provide a nice abstraction over several packages. In this notebook we explore using CMU Sphinx (an offline model, i.e. running locally), and Google (an offline model, i.e. over the network/cloud), but through SpeechRecognition package APIs.\n",
        "\n",
        "## Setup\n",
        "\n",
        "We need to install SpeechRecognition and pocketsphinx python packages, and download some files to test these APIs.\n",
        "\n",
        "1. **Install SpeechRecognition py package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0rokUuby2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install SpeechRecognition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpFEoLqdOd_3",
        "colab_type": "text"
      },
      "source": [
        "2. **Install Pocketsphinx**\n",
        "\n",
        "[Pocketsphinx](https://pypi.org/project/pocketsphinx/) is python bindings for [CMU Sphinx](https://cmusphinx.github.io/), and is one of the recognizer supported by SpeechRecognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZy7vLCKTQIp",
        "colab_type": "text"
      },
      "source": [
        "On MacOS: (using homebew):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgpe0kWlTTGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!brew install swig\n",
        "!swig -version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-BnUq7KTJSF",
        "colab_type": "text"
      },
      "source": [
        "On Linux:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58gUVIL7Q5K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y swig libpulse-dev\n",
        "!swig -version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tom0Jb8YVEkZ",
        "colab_type": "text"
      },
      "source": [
        "Now pip install poocketsphinx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_OHx87yP52E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install pocketsphinx\n",
        "!pip3 list | grep pocketsphinx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIB_P7CXey4",
        "colab_type": "text"
      },
      "source": [
        "## SpeechRecognition API\n",
        "\n",
        "SpeechRecognition has only batch API. First step to create an audio record, eithher from a file or from mic, and the second step is to call `recognize_<speech engine name>()` function. It currently has APIs for [CMU Sphinx, Google, Microsoft, IBM, Houndify, and Wit](https://github.com/Uberi/speech_recognition)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aia5lFgb-vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import speech_recognition as sr\n",
        "from enum import Enum, unique\n",
        "\n",
        "\n",
        "@unique\n",
        "class ASREngine(Enum):\n",
        "    sphinx = 0\n",
        "    google = 1\n",
        "\n",
        "\n",
        "def speech_to_text(filename: str, engine: ASREngine, language: str, show_all: bool = False) -> str:\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(filename) as source:\n",
        "        audio = r.record(source)\n",
        "\n",
        "    asr_functions = {\n",
        "        ASREngine.sphinx: r.recognize_sphinx,\n",
        "        ASREngine.google: r.recognize_google,\n",
        "    }\n",
        "\n",
        "    response = asr_functions[engine](audio, language=language, show_all=show_all)\n",
        "    return response\n",
        "\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    filename = t['filename']\n",
        "    text = t['text']\n",
        "    lang = t['lang']\n",
        "\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(filename, text))\n",
        "    for asr_engine in ASREngine:\n",
        "        try:\n",
        "            response = speech_to_text(filename, asr_engine, language=lang)\n",
        "            print('{0}: \"{1}\"'.format(asr_engine.name, response))\n",
        "        except sr.UnknownValueError:\n",
        "            print('{0} could not understand audio'.format(asr_engine.name))\n",
        "        except sr.RequestError as e:\n",
        "            print('{0} error: {0}'.format(asr_engine.name, e))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4V_G42z4DVp",
        "colab_type": "text"
      },
      "source": [
        "Notice the difference in results from sniphix and google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66lLoLCaL_nE",
        "colab_type": "text"
      },
      "source": [
        "## API for other providers\n",
        "\n",
        "For other speech recognition providers, you will need to create API credentials, which you have to pass to `recognize_<speech engine name>()`, you can checkout [this example](https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py).\n",
        "\n",
        "It also has a nice abstraction for Microphone, implemented over PyAudio/PortAudio. Here is an example to capture input from mic in [batch](https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py) and continously in [background](https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxzLeJZO4dHQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "You are not master of APIs of various speech recognition packages.\n"
      ]
    }
  ]
}